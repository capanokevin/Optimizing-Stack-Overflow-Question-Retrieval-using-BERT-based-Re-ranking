{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizing Stack Overflow Question Retrieval using BERT-based Re-ranking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Useful libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import spacy\n",
    "spacy.prefer_gpu()\n",
    "import re\n",
    "import joblib\n",
    "import numpy as np\n",
    "\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import torch\n",
    "import nltk\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.neighbors import KDTree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if GPU is available in torch\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "# check if GPU is available in keras\n",
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting data from web pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>codice sorgente</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>189602</td>\n",
       "      <td>[&lt;!DOCTYPE html&gt;\\n\\n&lt;html class=\"html__respons...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>189603</td>\n",
       "      <td>[&lt;!DOCTYPE html&gt;\\n\\n&lt;html class=\"html__respons...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>189604</td>\n",
       "      <td>[&lt;!DOCTYPE html&gt;\\n\\n&lt;html class=\"html__respons...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>189605</td>\n",
       "      <td>[&lt;!DOCTYPE html&gt;\\n\\n&lt;html class=\"html__respons...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>189606</td>\n",
       "      <td>[&lt;!DOCTYPE html&gt;\\n\\n&lt;html class=\"html__respons...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       id                                    codice sorgente\n",
       "0  189602  [<!DOCTYPE html>\\n\\n<html class=\"html__respons...\n",
       "1  189603  [<!DOCTYPE html>\\n\\n<html class=\"html__respons...\n",
       "2  189604  [<!DOCTYPE html>\\n\\n<html class=\"html__respons...\n",
       "3  189605  [<!DOCTYPE html>\\n\\n<html class=\"html__respons...\n",
       "4  189606  [<!DOCTYPE html>\\n\\n<html class=\"html__respons..."
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load web pages data\n",
    "stack_data_raw = r\"path to web pages data\"\n",
    "stack_data_df = pd.read_csv(stack_data_raw)\n",
    "stack_data_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>header</th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "      <th>tags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>189602</td>\n",
       "      <td>[&lt;!DOCTYPE html&gt;\\n\\n&lt;html class=\"html__respons...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>189603</td>\n",
       "      <td>[&lt;!DOCTYPE html&gt;\\n\\n&lt;html class=\"html__respons...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>189604</td>\n",
       "      <td>[&lt;!DOCTYPE html&gt;\\n\\n&lt;html class=\"html__respons...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>189605</td>\n",
       "      <td>[&lt;!DOCTYPE html&gt;\\n\\n&lt;html class=\"html__respons...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>189606</td>\n",
       "      <td>[&lt;!DOCTYPE html&gt;\\n\\n&lt;html class=\"html__respons...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   header                                           question answer  tags\n",
       "0  189602  [<!DOCTYPE html>\\n\\n<html class=\"html__respons...   None  None\n",
       "1  189603  [<!DOCTYPE html>\\n\\n<html class=\"html__respons...   None  None\n",
       "2  189604  [<!DOCTYPE html>\\n\\n<html class=\"html__respons...   None  None\n",
       "3  189605  [<!DOCTYPE html>\\n\\n<html class=\"html__respons...   None  None\n",
       "4  189606  [<!DOCTYPE html>\\n\\n<html class=\"html__respons...   None  None"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# creating 2 new columns for tags and answers\n",
    "stack_data_df['answer'] = None\n",
    "stack_data_df['tags'] = None\n",
    "\n",
    "stack_data_df = stack_data_df.rename(columns={'id': 'header', 'codice sorgente': 'question'})\n",
    "stack_data_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kevin\\AppData\\Local\\Temp\\ipykernel_50400\\3869076605.py:11: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  stack_data_df['answer'][idx] = answer.text\n",
      "C:\\Users\\kevin\\AppData\\Local\\Temp\\ipykernel_50400\\3869076605.py:19: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  stack_data_df['header'][idx] = header.text\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In this scraping process we have 19 of 10397 question/answer that were dropped because of missing information\n"
     ]
    }
   ],
   "source": [
    "# extracting question, context question, accepted answer, tags from html\n",
    "missing_info = 0\n",
    "\n",
    "for idx,soup in enumerate(stack_data_df['question']):\n",
    "\n",
    "    soup = BeautifulSoup(soup,\"html.parser\")\n",
    "\n",
    "    try:\n",
    "      # Getting the answer\n",
    "      answer = soup.find('div', class_ = ['answer', 'js-answer'], itemprop='acceptedAnswer') \n",
    "      answer = answer.find('div', class_ = ['s-prose js-post-body'], itemprop='text')   \n",
    "      stack_data_df['answer'][idx] = answer.text\n",
    "\n",
    "    except:\n",
    "      missing_info+=1\n",
    "      continue\n",
    "    \n",
    "    # Getting the header\n",
    "    header = soup.find('h1', class_ = ['fs-headline1 ow-break-word mb8 flex--item fl1'], itemprop ='name')\n",
    "    stack_data_df['header'][idx] = header.text\n",
    "\n",
    "    # Getting the question\n",
    "    question = soup.find('div', class_ = ['question js-question'], id ='question') \n",
    "    question = question.find('div', class_ = ['s-prose js-post-body'], itemprop='text') \n",
    "    stack_data_df['question'][idx] = question.text\n",
    "\n",
    "    # Getting the tags\n",
    "    tags = soup.find('div', class_ = ['d-flex ps-relative fw-wrap'])\n",
    "    tags = tags.text.split()[1:]\n",
    "    # se è missing riempio con lista vuota\n",
    "    if len(tags) == 0:\n",
    "      tags = []\n",
    "    stack_data_df['tags'][idx] =  tags\n",
    "\n",
    "stack_data_df.to_csv(fr'C:\\Users\\kevin\\Desktop\\Text Mining data\\df_21.csv', index = False)\n",
    "print(f\"In this scraping process we have {missing_info} of {idx} question/answer that were dropped because of missing information\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-Processing: from raw data to cleaned text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleaning the question \n",
    "# 1. lower text\n",
    "# 2. remove the new line character\n",
    "df['cleaned_question'] = df['question'].apply(lambda x:x.lower().replace('\\n', ''))\n",
    "df['cleaned_answer'] = df['answer'].apply(lambda x:str(x).lower().replace('\\n', ''))\n",
    "df['cleaned_header'] = df['header'].apply(lambda x:str(x).lower().replace('\\n', ''))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(244095, 7)\n",
      "(244095, 7)\n"
     ]
    }
   ],
   "source": [
    "# drop rows with nan question or answer\n",
    "print(df.shape)\n",
    "df.dropna(subset=['question', 'answer', 'header'], inplace=True)\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: [header, question, answer, tags, cleaned_question, cleaned_answer]\n",
      "Index: []\n",
      "Empty DataFrame\n",
      "Columns: [header, question, answer, tags, cleaned_question, cleaned_answer]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "# print rows with missing question or answer: OKS, no missing\n",
    "print(df[df['question'].isna()])\n",
    "print(df[df['answer'].isna()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Expand contractions\n",
    "\n",
    "contractions_dict = { \"ain't\": \"are not\",\"'s\":\" is\",\"aren't\": \"are not\",\"can't\": \"can not\",\"can't've\": \"cannot have\",\n",
    "\"'cause\": \"because\",\"could've\": \"could have\",\"couldn't\": \"could not\",\"couldn't've\": \"could not have\",\n",
    "\"didn't\": \"did not\",\"doesn't\": \"does not\",\"don't\": \"do not\",\"hadn't\": \"had not\",\"hadn't've\": \"had not have\",\n",
    "\"hasn't\": \"has not\",\"haven't\": \"have not\",\"he'd\": \"he would\",\"he'd've\": \"he would have\",\"he'll\": \"he will\",\n",
    "\"he'll've\": \"he will have\",\"how'd\": \"how did\",\"how'd'y\": \"how do you\",\"how'll\": \"how will\",\"i'd\": \"i would\",\n",
    "\"i'd've\": \"i would have\",\"i'll\": \"i will\",\"i'll've\": \"i will have\",\"i'm\": \"i am\",\"i've\": \"i have\",\n",
    "\"isn't\": \"is not\",\"it'd\": \"it would\",\"it'd've\": \"it would have\",\"it'll\": \"it will\",\"it'll've\": \"it will have\",\n",
    "\"let's\": \"let us\",\"ma'am\": \"madam\",\"mayn't\": \"may not\",\"might've\": \"might have\",\"mightn't\": \"might not\",\n",
    "\"mightn't've\": \"might not have\",\"must've\": \"must have\",\"mustn't\": \"must not\",\"mustn't've\": \"must not have\",\n",
    "\"needn't\": \"need not\",\"needn't've\": \"need not have\",\"o'clock\": \"of the clock\",\"oughtn't\": \"ought not\",\n",
    "\"oughtn't've\": \"ought not have\",\"shan't\": \"shall not\",\"sha'n't\": \"shall not\",\n",
    "\"shan't've\": \"shall not have\",\"she'd\": \"she would\",\"she'd've\": \"she would have\",\"she'll\": \"she will\",\n",
    "\"she'll've\": \"she will have\",\"should've\": \"should have\",\"shouldn't\": \"should not\",\n",
    "\"shouldn't've\": \"should not have\",\"so've\": \"so have\",\"that'd\": \"that would\",\"that'd've\": \"that would have\",\n",
    "\"there'd\": \"there would\",\"there'd've\": \"there would have\",\n",
    "\"they'd\": \"they would\",\"they'd've\": \"they would have\",\"they'll\": \"they will\",\"they'll've\": \"they will have\",\n",
    "\"they're\": \"they are\",\"they've\": \"they have\",\"to've\": \"to have\",\"wasn't\": \"was not\",\"we'd\": \"we would\",\n",
    "\"we'd've\": \"we would have\",\"we'll\": \"we will\",\"we'll've\": \"we will have\",\"we're\": \"we are\",\"we've\": \"we have\",\n",
    "\"weren't\": \"were not\",\"what'll\": \"what will\",\"what'll've\": \"what will have\",\"what're\": \"what are\",\n",
    "\"what've\": \"what have\",\"when've\": \"when have\",\"where'd\": \"where did\",\n",
    "\"where've\": \"where have\",\"who'll\": \"who will\",\"who'll've\": \"who will have\",\"who've\": \"who have\",\n",
    "\"why've\": \"why have\",\"will've\": \"will have\",\"won't\": \"will not\",\"won't've\": \"will not have\",\n",
    "\"would've\": \"would have\",\"wouldn't\": \"would not\",\"wouldn't've\": \"would not have\",\"y'all\": \"you all\",\n",
    "\"y'all'd\": \"you all would\",\"y'all'd've\": \"you all would have\",\"y'all're\": \"you all are\",\"y'all've\": \"you all have\",\n",
    "\"you'd\": \"you would\",\"you'd've\": \"you would have\",\"you'll\": \"you will\",\"you'll've\": \"you will have\",\n",
    "\"you're\": \"you are\",\"you've\": \"you have\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regular expression for finding contractions\n",
    "contractions_re=re.compile('(%s)' % '|'.join(contractions_dict.keys()))\n",
    "\n",
    "# Function for expanding contractions\n",
    "def expand_contractions(text,contractions_dict=contractions_dict):\n",
    "    def replace(match):\n",
    "        return contractions_dict[match.group(0)]\n",
    "    return contractions_re.sub(replace, text)\n",
    "\n",
    "# Expanding Contractions\n",
    "df['cleaned_question'] = df['cleaned_question'].apply(lambda x:expand_contractions(x))\n",
    "df['cleaned_answer'] = df['cleaned_answer'].apply(lambda x:expand_contractions(x))\n",
    "df['cleaned_header'] = df['cleaned_header'].apply(lambda x:expand_contractions(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "I'm a beginner at Python and I don't know what to set command to so I can open one of the links in \n",
      "i am a beginner at python and i do not know what to set command to so i can open one of the links in\n"
     ]
    }
   ],
   "source": [
    "print(df['question'][4][0:100])\n",
    "print(df['cleaned_question'][4][0:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for Cleaning Text\n",
    "# For cleaning the documents, I have created a function clean_text() which will remove:\n",
    "# 1. the words with digits\n",
    "# 2. replace newline characters with space, \n",
    "# 3. remove URLs \n",
    "# 4. replace everything that isn’t English alphabets with space\n",
    "\n",
    "def clean_text(text):\n",
    "    text = re.sub('\\w*\\d\\w*','', text)\n",
    "    #text=re.sub('\\n',' ',text)\n",
    "    text = re.sub(r\"http\\S+\", \"\", text)\n",
    "    text = re.sub('[^a-z]',' ',text)\n",
    "    return text\n",
    "\n",
    "# Cleaning corpus using RegEx\n",
    "df['cleaned_question'] = df['cleaned_question'].apply(lambda x: clean_text(x))\n",
    "df['cleaned_answer'] = df['cleaned_answer'].apply(lambda x: clean_text(x))\n",
    "df['cleaned_header'] = df['cleaned_header'].apply(lambda x: clean_text(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Use floor division:\n",
      ">>> 111111111111111111//10\n",
      "11111111111111111\n",
      ">>> \n",
      "\n",
      "\n",
      "use floor division           \n"
     ]
    }
   ],
   "source": [
    "print(df['answer'][2][0:100])\n",
    "print(df['cleaned_answer'][2][0:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing extra spaces\n",
    "df['cleaned_question'] = df['cleaned_question'].apply(lambda x: re.sub(' +',' ',x))\n",
    "df['cleaned_answer'] = df['cleaned_answer'].apply(lambda x: re.sub(' +',' ',x))\n",
    "df['cleaned_header'] = df['cleaned_header'].apply(lambda x: re.sub(' +',' ',x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I have two dataframes with different indexing that I want to sum the same column from the two dataframes. Based on a suggestion, I tried the following but removes disregards other columns like catdf = df.set_index('date')tmp = tmp.set_index('date')result = df['Anomaly'].add(tmp['Anomaly'], fill_value=0).reset_index()df    date       cat    Anomaly0 2018-12-06    a      01 2019-01-07    b      02 2019-02-06    a      13 2019-03-06    a      04 2019-04-06    b      0tmp    date        cat   Anomaly0 2018-12-06     a      01 2019-01-07     b      14 2019-04-06     b      0result    date           Anomaly0 2018-12-06         0.01 2019-01-07         1.0 2 2019-02-06         1.03 2019-03-06         0.04 2019-04-06         0.0What I want actually is to sum based on index and keep the category column and int dtype of Anomaly:result    date          cat    Anomaly0 2018-12-06       a         01 2019-01-07       b         12 2019-02-06       a         13 2019-03-06       a         04 2019-04-06       b         0\n",
      "--------------------------------------------------------\n",
      "i have two dataframes with different indexing that i want to sum the same column from the two dataframes based on a suggestion i tried the following but removes disregards other columns like catdf df set index date tmp tmp set index date result df anomaly add tmp anomaly fill value reset index df date cat a b a a b date cat a b b date i want actually is to sum based on index and keep the category column and int dtype of anomaly result date cat a b a a b \n"
     ]
    }
   ],
   "source": [
    "print(df['question'][200].replace('\\n',''))\n",
    "print('--------------------------------------------------------')\n",
    "print(df['cleaned_question'][200])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stopwords removal & Lemmatizing tokens using SpaCy\n",
    "# The goal of both stemming and lemmatization is to reduce inflectional forms and sometimes derivationally related forms of a word to a common base \n",
    "# form. However, the two words differ in their flavor. Stemming usually refers to a crude heuristic process that chops off the ends of words in the \n",
    "# hope of achieving this goal correctly most of the time, and often includes the removal of derivational affixes. Lemmatization usually refers to \n",
    "# doing things properly with the use of a vocabulary and morphological analysis of words, normally aiming to remove inflectional endings only and to \n",
    "# return the base or dictionary form of a word, which is known as the lemma.\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm',disable=['ner','parser'])\n",
    "nlp.max_length = 5000000\n",
    "\n",
    "\n",
    "# Removing Stopwords and Lemmatizing words\n",
    "df['lemmatized_question']=df['cleaned_question'].apply(lambda x: ' '.join([token.lemma_ for token in list(nlp(x)) if (token.is_stop==False)]))\n",
    "df['lemmatized_answer']=df['cleaned_answer'].apply(lambda x: ' '.join([token.lemma_ for token in list(nlp(x)) if (token.is_stop==False)]))\n",
    "df['lemmatized_header']=df['cleaned_header'].apply(lambda x: ' '.join([token.lemma_ for token in list(nlp(x)) if (token.is_stop==False)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"data_preprocessed.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word2Vec: from words to vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_data = pd.read_csv(\"data_preprocessed.csv\")\n",
    "pre_data.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(207480, 10) (36615, 10) (244095, 10)\n"
     ]
    }
   ],
   "source": [
    "# shuffle the data, divide into 3/4 training and 1/4 test\n",
    "pre_data = pre_data.sample(frac=1, random_state = 19091999).reset_index(drop=True)\n",
    "train = pre_data[:int(len(pre_data)*0.85)]\n",
    "test = pre_data[int(len(pre_data)*0.85):]\n",
    "print(train.shape, test.shape, pre_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "header                                    Sliding window reports caching\n",
       "question               \\nI have api endpoint that takes as an input s...\n",
       "answer                 \\nYou may consider using redis pipeline rather...\n",
       "tags                                       ['flask', 'caching', 'redis']\n",
       "cleaned_question       i have api endpoint that takes as an input som...\n",
       "cleaned_answer         you may consider using redis pipeline rather t...\n",
       "cleaned_header                            sliding window reports caching\n",
       "lemmatized_question    api endpoint take input datum date date field ...\n",
       "lemmatized_answer      consider redis pipeline individual command loo...\n",
       "lemmatized_header                              slide window report cache\n",
       "Name: 0, dtype: object"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['have', 'follow', 'code', 'def', 'choose', 'set', 'lst', 'k', 'k', 'return', 'len', 'lst', 'k', 'return', 'lst', 'return', 'choose', 'set', 'lst', 'k', 'lst', 'choose', 'set', 'lst', 'k', 'choose', 'set', 'lst', 'k', 'work', 'possible', 'loop', 'instead', 'write', 'lst', 'choose', 'set', 'lst', 'k', 'function', 'return', 'list', 'contain', 'different', 'list', 'length', 'k', 'create', 'original', 'list', 'member', 'order', 'important']\n"
     ]
    }
   ],
   "source": [
    "# Combining corpus and queries for training\n",
    "combined=pd.concat([train.rename(columns={'lemmatized_header':'text1'})['text1'],\\\n",
    "                             train.rename(columns={'lemmatized_question':'text2'})['text2'],\\\n",
    "                             train.rename(columns={'lemmatized_answer':'text3'})['text3']])\\\n",
    "                             .sample(frac=1).reset_index(drop=True)\n",
    "print(combined.shape)\n",
    "combined = combined.to_list()\n",
    "# Creating data for the model training\n",
    "corpus_tokens = [str(sentence).split() for sentence in combined]\n",
    "print(corpus_tokens[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training a word2vec model from the given data set\n",
    "w2v_model = Word2Vec(corpus_tokens, vector_size=500, min_count=2, window=5, sg=1, workers=8) \n",
    "\n",
    "# save the model\n",
    "w2v_model.save(r\"word2vec4.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(r'training.csv')\n",
    "test = pd.read_csv(r'test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTA: devono esserci tutti e 3 i file nella cartella Models (word2vec.model, word2vec.model.trainables.syn1neg.npy, word2vec.model.wv.vectors.npy)\n",
    "w2v_model = Word2Vec.load(r\"/home/kevin/workspace/text mining/Project/Models/word2vec_v4/word2vec4.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function returning vector reperesentation of a document: per ogni documento ritorna la media dei vettori delle parole che lo compongono\n",
    "def get_embedding_w2v(doc_tokens, w2v_model=w2v_model):\n",
    "    embeddings = []\n",
    "    if len(doc_tokens)<1:\n",
    "        print(\"collection is empty\")\n",
    "        return np.zeros(500)\n",
    "    else:\n",
    "        for tok in doc_tokens:\n",
    "            try:\n",
    "                embeddings.append(w2v_model.wv[tok])\n",
    "                #print(\"embedding riuscito per: \", tok)\n",
    "            except:\n",
    "                embeddings.append(np.random.rand(500))\n",
    "                print(\"embedding fallito per: \", tok)\n",
    "                \n",
    "        # mean the vectors of individual words to get the vector of the document\n",
    "        return np.mean(embeddings, axis=0) \n",
    "\n",
    "# Getting Word2Vec Vectors for Testing Corpus and Queries\n",
    "test['answer_vector'] = test['lemmatized_answer'].apply(lambda x : get_embedding_w2v(str(x).split(), w2v_model))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create KDTree for fast retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['kdtree_word2vec_from_scratch2.joblib']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer_retrievals = np.array(test['answer_vector'].tolist())\n",
    "tree = KDTree(answer_retrievals) \n",
    "\n",
    "joblib.dump(tree, \"kdtree_word2vec_from_scratch2.joblib\")\n",
    "tree = joblib.load(\"kdtree_word2vec_from_scratch2.joblib\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ranking_ir(query, tree, data, k):\n",
    "  \n",
    "  # pre-process Query\n",
    "  query=query.lower().replace('\\n', '')\n",
    "  query=expand_contractions(query)\n",
    "  query=clean_text(query)\n",
    "  query=re.sub(' +',' ', query)\n",
    "  query = ' '.join([token.lemma_ for token in list(nlp(query)) if (token.is_stop==False)])\n",
    "\n",
    "  # generating vector\n",
    "  vector = get_embedding_w2v(query.split())\n",
    "\n",
    "  # ranking documents\n",
    "  dist, ind = tree.query(np.expand_dims(vector, axis = 0), k = k)   \n",
    "  \n",
    "  return data.iloc[ind.tolist()[0]][['cleaned_answer', 'tags']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recall@5, Recall@10, Recall@20\n",
    "recall5 = []\n",
    "recall10 = []\n",
    "recall20 = []\n",
    "for idx, question in enumerate(test['header']):\n",
    "    if idx % 1000 == 0:\n",
    "        print(idx)\n",
    "    results = ranking_ir(question, tree, test, 20)\n",
    "    if idx in results[0:5].index.tolist():\n",
    "        recall5.append(1)\n",
    "    if idx in results[0:10].index.tolist():\n",
    "        recall10.append(1)\n",
    "    if idx in results[0:20].index.tolist():\n",
    "        recall20.append(1)\n",
    "recall5_result = sum(recall5)/len(test['header'])\n",
    "recall10_result = sum(recall10)/len(test['header'])\n",
    "recall20_result = sum(recall20)/len(test['header'])\n",
    "print(\"Recall@5: \", recall5_result)\n",
    "print(\"Recall@10: \", recall10_result)\n",
    "print(\"Recall@20: \", recall20_result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BERT: training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def augment_train(train):\n",
    "  new_train = pd.DataFrame({'header': [], 'answer':[], 'label':[]})\n",
    "  for idx, header in enumerate(train['cleaned_header'].values):\n",
    "    if idx % 1000 == 0:\n",
    "      print(idx)\n",
    "    results = ranking_ir(header, tree,test, 9)\n",
    "    rand_answers = list(results['cleaned_answer'].values)\n",
    "    rand_answers.insert(0, train['cleaned_answer'][idx])\n",
    "    headers = [header]*9\n",
    "    labels = [1,0,0,0,0,0,0,0,0,0]\n",
    "    current_df = pd.DataFrame({'header': headers, 'answer': rand_answers, 'label':labels})\n",
    "    new_train = pd.concat([new_train, current_df], ignore_index = True)\n",
    "  return new_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_aug2 = augment_train(train)\n",
    "train_aug2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>header</th>\n",
       "      <th>answer</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>sliding window reports caching</td>\n",
       "      <td>you may consider using redis pipeline rather t...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>sliding window reports caching</td>\n",
       "      <td>see below a corrected version of your code exp...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>sliding window reports caching</td>\n",
       "      <td>use a transaction with spkdb transaction for k...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>sliding window reports caching</td>\n",
       "      <td>fo open headervalue txt r data l split for l i...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>sliding window reports caching</td>\n",
       "      <td>had this same issue myself once i had worked i...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>sliding window reports caching</td>\n",
       "      <td>try this import rewith open results txt as inf...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>sliding window reports caching</td>\n",
       "      <td>askopenfilename returns a string containing th...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>sliding window reports caching</td>\n",
       "      <td>there are too many questions on this topic you...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>sliding window reports caching</td>\n",
       "      <td>this seems like an example of a repeated probl...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>sliding window reports caching</td>\n",
       "      <td>i solved my question i was looking to implemen...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>find all files in a directory with extension t...</td>\n",
       "      <td>you can use glob import glob osos chdir mydir ...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>find all files in a directory with extension t...</td>\n",
       "      <td>for anyone who has stumbled upon this question...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>find all files in a directory with extension t...</td>\n",
       "      <td>you will need to read up on mod python if you ...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>find all files in a directory with extension t...</td>\n",
       "      <td>you can use dump to write to a new file json d...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>find all files in a directory with extension t...</td>\n",
       "      <td>the python package for protobuf has a package ...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               header  \\\n",
       "0                      sliding window reports caching   \n",
       "1                      sliding window reports caching   \n",
       "2                      sliding window reports caching   \n",
       "3                      sliding window reports caching   \n",
       "4                      sliding window reports caching   \n",
       "5                      sliding window reports caching   \n",
       "6                      sliding window reports caching   \n",
       "7                      sliding window reports caching   \n",
       "8                      sliding window reports caching   \n",
       "9                      sliding window reports caching   \n",
       "10  find all files in a directory with extension t...   \n",
       "11  find all files in a directory with extension t...   \n",
       "12  find all files in a directory with extension t...   \n",
       "13  find all files in a directory with extension t...   \n",
       "14  find all files in a directory with extension t...   \n",
       "\n",
       "                                               answer  label  \n",
       "0   you may consider using redis pipeline rather t...    1.0  \n",
       "1   see below a corrected version of your code exp...    0.0  \n",
       "2   use a transaction with spkdb transaction for k...    0.0  \n",
       "3   fo open headervalue txt r data l split for l i...    0.0  \n",
       "4   had this same issue myself once i had worked i...    0.0  \n",
       "5   try this import rewith open results txt as inf...    0.0  \n",
       "6   askopenfilename returns a string containing th...    0.0  \n",
       "7   there are too many questions on this topic you...    0.0  \n",
       "8   this seems like an example of a repeated probl...    0.0  \n",
       "9   i solved my question i was looking to implemen...    0.0  \n",
       "10  you can use glob import glob osos chdir mydir ...    1.0  \n",
       "11  for anyone who has stumbled upon this question...    0.0  \n",
       "12  you will need to read up on mod python if you ...    0.0  \n",
       "13  you can use dump to write to a new file json d...    0.0  \n",
       "14  the python package for protobuf has a package ...    0.0  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "train = pd.read_csv('train_aug.csv')\n",
    "train.head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we sample the negative examples in order to insert some noise in the data, otherwise the model predict always the most relevant class\n",
    "\n",
    "import pandas as pd\n",
    "train = pd.read_csv('train_aug.csv')\n",
    "\n",
    "# Select the rows with label 0\n",
    "zero_rows = train[train['label'] == 0]\n",
    "\n",
    "# Randomly select 50% of the rows with label 0\n",
    "random_zero_rows = zero_rows.sample(frac=0.7)\n",
    "\n",
    "# Get the index values of the selected rows\n",
    "random_zero_row_indices = random_zero_rows.index\n",
    "\n",
    "# Drop the selected rows from the DataFrame\n",
    "train = train.drop(random_zero_row_indices)\n",
    "\n",
    "# reset the index\n",
    "train = train.reset_index(drop = True)\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "test = train.iloc[700000:767676]  # 767676 length\n",
    "train = train.iloc[:700000]\n",
    "\n",
    "\n",
    "train = train.reset_index(drop = True)\n",
    "print(train.shape, test.shape)\n",
    "\n",
    "train.tail(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0    0.729775\n",
       "1.0    0.270225\n",
       "Name: label, dtype: float64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# count the number of 0 and 1 labels\n",
    "train['label'].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Il training del modello è stato spezzato in 3 fasi, quindi si è ottenuto 1 modello ogni 2 epoche. Per questa ragione si è caricato il modello dopo 2 epoche \n",
    "# e si è continuato il training per altre 2 epoche per 2 volte.\n",
    "\n",
    "from transformers import BertModel, BertTokenizer\n",
    "import torch\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Load the model\n",
    "model = BertModel.from_pretrained('bert-base-uncased').to(device)\n",
    "model.load_state_dict(torch.load(r'Models/BERT/v3/model3.pt'))\n",
    "\n",
    "# Load the classification head\n",
    "classification_head = torch.nn.Sequential(torch.nn.Dropout(0.4), torch.nn.Linear(768, 1),torch.nn.Sigmoid()).to(device)\n",
    "classification_head.load_state_dict(torch.load(r'Models/BERT/v3/classification_head3.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertModel, BertTokenizer\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "import torch\n",
    "import warnings\n",
    "import numpy as np\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "    model = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Set the number of epochs and the batch size\n",
    "num_epochs = 2\n",
    "batch_size = 100\n",
    "\n",
    "# Set the device to use for training\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# freeze the BERT model\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Instantiate the tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "## Model e Classification head sono commmentati poichè il training è stato spezzato in 3 fasi, quindi il modello è stato salvato e caricato in seguito\n",
    "# send the model to the device\n",
    "#model = BertClassificationModel()\n",
    "#model = model.to(device)\n",
    "\n",
    "# Add a classification head on top of the BERT model\n",
    "#classification_head = torch.nn.Sequential(torch.nn.Dropout(0.4), torch.nn.Linear(768, 1),torch.nn.Sigmoid()).to(device)\n",
    "\n",
    "\n",
    "# Set the seed value all over the place to make this reproducible.\n",
    "seed_val = 42\n",
    "\n",
    "random.seed(seed_val)\n",
    "np.random.seed(seed_val)\n",
    "torch.manual_seed(seed_val)\n",
    "torch.cuda.manual_seed_all(seed_val)\n",
    "\n",
    "\n",
    "\n",
    "# Set the optimizer and the loss function\n",
    "optimizer = torch.optim.Adam(classification_head.parameters(), lr=2e-5, eps=1e-8)  #classification_head.parameters() se uso la head separata e non il modello\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, \n",
    "                                            num_warmup_steps = 0, # Default value in run_glue.py\n",
    "                                            num_training_steps = 700000)\n",
    "loss_fn = torch.nn.BCEWithLogitsLoss()\n",
    "\n",
    "# Track the number of correct predictions\n",
    "accuracy_test = []\n",
    "precision_test = []\n",
    "recall_test = []\n",
    "f1_test = []\n",
    "total_train_loss = []\n",
    "total_test_loss = []\n",
    "\n",
    "skipped_train = 0\n",
    "skipped_test = 0\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "\n",
    "    # Shuffle the training data\n",
    "    train = train.sample(frac=1).reset_index(drop=True)   \n",
    "\n",
    "    # Reset the total loss for this epoch.\n",
    "    train_epoch_loss = 0\n",
    "    test_epoch_loss = 0\n",
    "    \n",
    "    model.train()\n",
    "\n",
    "    # Split the training data into batches\n",
    "    num_batches = (len(train) // batch_size) + 1\n",
    "\n",
    "    for i in range(num_batches):\n",
    "        # Get the next batch of data\n",
    "        print('TRAIN:', i)\n",
    "        batch = train[i * batch_size:(i + 1) * batch_size]\n",
    "        try:\n",
    "            encoding = tokenizer(\n",
    "                        text = batch['header'].to_list(),\n",
    "                        text_pair = batch['answer'].to_list(),\n",
    "                        add_special_tokens = True,\n",
    "                        max_length = 512,\n",
    "                        padding  = True,\n",
    "                        return_attention_mask = True,\n",
    "                        return_tensors = 'pt',\n",
    "                        truncation = True\n",
    "                   ).to(device)\n",
    "        except:\n",
    "            skipped_train += 1        # Skip the batch if there is an error\n",
    "            continue \n",
    "\n",
    "        # Zero the gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass through the model\n",
    "        ids = encoding['input_ids']\n",
    "        mask = encoding['attention_mask']\n",
    "        token_type_ids = encoding['token_type_ids']\n",
    "        output = model(ids, mask, token_type_ids)[0]\n",
    "        output = output[:, 0, :] + output[:, 1, :]   # take the first and last token embeddings and sum them up\n",
    "        output = classification_head(output) \n",
    "\n",
    "        # Compute the loss\n",
    "        labels = torch.tensor(batch['label'].values).float().to(device)\n",
    "        loss = loss_fn(output.squeeze(), labels)\n",
    "        train_epoch_loss += loss.item()\n",
    "\n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "\n",
    "        # Clip the norm of the gradients to 1.0.\n",
    "        # This is to help prevent the \"exploding gradients\" problem.\n",
    "        torch.nn.utils.clip_grad_norm_(classification_head.parameters(), 1.0)\n",
    "\n",
    "        # Update the model's weights\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "\n",
    "    # udate total training loss\n",
    "    total_train_loss.append(train_epoch_loss / num_batches)\n",
    "\n",
    "\n",
    "    # Calculate the accuracy on the validation set\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        num_correct = 0\n",
    "        num_true_positives = 0\n",
    "        num_false_positives = 0\n",
    "        num_true_negatives = 0\n",
    "        num_false_negatives = 0\n",
    "        num_total = 0\n",
    "        num_batches_test = (len(test) // batch_size) + 1\n",
    "\n",
    "        for i in range(num_batches_test):\n",
    "            print('TEST:', i)\n",
    "            batch_test = test[i * batch_size : (i + 1) * batch_size]\n",
    "            try: \n",
    "                encoding = tokenizer(\n",
    "                        text = batch_test['header'].to_list(),\n",
    "                        text_pair = batch_test['answer'].to_list(),\n",
    "                        add_special_tokens = True,\n",
    "                        max_length = 512,\n",
    "                        padding  = True,\n",
    "                        return_attention_mask = True,\n",
    "                        return_tensors = 'pt',\n",
    "                        truncation = True\n",
    "                   ).to(device)\n",
    "            except:\n",
    "                skipped_test += 1\n",
    "                continue  \n",
    "\n",
    "            ids = encoding['input_ids']\n",
    "            mask = encoding['attention_mask']\n",
    "            token_type_ids = encoding['token_type_ids']\n",
    "\n",
    "            output = model(ids, mask, token_type_ids)[0]\n",
    "            output = output[:, 0, :] + output[:, 1, :]\n",
    "            output = classification_head(output)\n",
    "\n",
    "            # Compute the loss\n",
    "            labels = torch.tensor(batch_test['label'].values).float().to(device)\n",
    "            loss = loss_fn(output.squeeze(), labels)\n",
    "            test_epoch_loss += loss.item()\n",
    "\n",
    "            # Compute the accuracy\n",
    "            predicted_label = (output > 0.5).long()                                           # long: trasforma vettore Boolean in 0 e 1\n",
    "            label = torch.tensor(batch_test['label'].values).to(device)[:, np.newaxis]\n",
    "            num_correct += (predicted_label == label).sum().item()\n",
    "\n",
    "            # Calculate the true and false positives and negatives\n",
    "            num_true_positives += ((predicted_label == 1) & (label == 1)).sum().item()\n",
    "            num_true_negatives += ((predicted_label == 0) & (label == 0)).sum().item()\n",
    "            num_false_positives += ((predicted_label == 1) & (label == 0)).sum().item()\n",
    "            num_false_negatives += ((predicted_label == 0) & (label == 1)).sum().item()\n",
    "            \n",
    "            \n",
    "        # update total test loss\n",
    "        total_test_loss.append(test_epoch_loss / num_batches_test)\n",
    "\n",
    "        # Calculate the accuracy\n",
    "        accuracy = num_correct / (len(test) - skipped_test)\n",
    "        accuracy_test.append(accuracy)\n",
    "\n",
    "        # Compute precision\n",
    "        precision = num_true_positives / (num_true_positives + num_false_positives)\n",
    "        precision_test.append(precision)\n",
    "\n",
    "        # Compute recall\n",
    "        recall = num_true_positives / (num_true_positives + num_false_negatives)\n",
    "        recall_test.append(recall)\n",
    "\n",
    "        # Compute f1\n",
    "        f1 = 2 * precision * recall / (precision + recall)\n",
    "        f1_test.append(f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: [0.872637452749055, 0.8729949197967919]\n",
      "Precision [0.897346525747695, 0.894356005788712]\n",
      "Recall [0.5939569844459329, 0.5979013172583166]\n",
      "F1 [0.7147910975773587, 0.7166815343443355]\n"
     ]
    }
   ],
   "source": [
    "print(f'Accuracy: {accuracy_test}')\n",
    "print('Precision', precision_test)\n",
    "print('Recall', recall_test)\n",
    "print('F1', f1_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the model\n",
    "torch.save(model.state_dict(), r'Models/BERT/v4/model4.pt')\n",
    "# save the classification head\n",
    "torch.save(classification_head.state_dict(), r'Models/BERT/v4/classification_head4.pt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "# Plot the training and validation loss\n",
    "plt.plot(total_train_loss, label='Training loss')\n",
    "plt.plot(total_test_loss, label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "# Plot the training and validation loss\n",
    "plt.plot(accuracy_test, label='Accuracy')\n",
    "plt.title('Validation accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BERT: re-ranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import BertModel, BertTokenizer\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "import torch\n",
    "import warnings\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Load the model\n",
    "model = BertModel.from_pretrained('bert-base-uncased').to(device)\n",
    "model.load_state_dict(torch.load(r'/home/kevin/workspace/text mining/Project/Models/BERT/v4/model4.pt'))\n",
    "\n",
    "# tokenizer \n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Load the classification head\n",
    "classification_head = torch.nn.Sequential(torch.nn.Dropout(0.4), torch.nn.Linear(768, 1),torch.nn.Sigmoid()).to(device)\n",
    "classification_head.load_state_dict(torch.load(r'/home/kevin/workspace/text mining/Project/Models/BERT/v4/classification_head4.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put the model in evaluation mode--the dropout layers behave differently\n",
    "# during evaluation.\n",
    "model.eval()\n",
    "\n",
    "num_precision_5 = 0\n",
    "num_precision_10 = 0\n",
    "num_precision_20 = 0\n",
    "\n",
    "# precision lists\n",
    "prec_5 = []\n",
    "prec_10 = []\n",
    "prec_20 = []\n",
    "\n",
    "# import the test set\n",
    "#test = pd.read_csv('test.csv')\n",
    "average_precisions = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "\n",
    "    for idx, question in enumerate(test['header']):\n",
    "        # get the top 1000 results\n",
    "        results = ranking_ir(question, tree, test, 1000)\n",
    "\n",
    "        # take the questions and put them in a list\n",
    "        answers = results['cleaned_answer'].to_list()\n",
    "        \n",
    "        # create a list with 1000 times the same question\n",
    "        questions = [question] * 125\n",
    "\n",
    "        scores = []\n",
    "        for i in range(8):\n",
    "            encoding = tokenizer(\n",
    "                text = questions,\n",
    "                text_pair = answers[i*125:(i+1)*125],\n",
    "                add_special_tokens = True,\n",
    "                max_length = 512,\n",
    "                padding  = True,\n",
    "                return_attention_mask = True,\n",
    "                return_tensors = 'pt',\n",
    "                truncation = True\n",
    "            ).to(device)\n",
    "\n",
    "            ids = encoding['input_ids']\n",
    "            mask = encoding['attention_mask']\n",
    "            token_type_ids = encoding['token_type_ids']\n",
    "\n",
    "            output = model(ids, mask, token_type_ids)[0]\n",
    "            output = output[:, 0, :] + output[:, 1, :]\n",
    "            output = classification_head(output)\n",
    "            scores.append(output.squeeze().cpu().numpy())\n",
    "\n",
    "        scores = np.array(scores).flatten()\n",
    "        results['score'] = scores\n",
    "        results = results.sort_values(by=['score'], ascending=False)\n",
    "\n",
    "        # check if test['cleaned_answer'][idx] is in the top 5,10 or 20 results\n",
    "        if idx in results[0:5].index.tolist():\n",
    "            num_precision_5 += 1\n",
    "        if idx in results[0:10].index.tolist():\n",
    "            num_precision_10 += 1\n",
    "        if idx in results[0:20].index.tolist():\n",
    "            num_precision_20 += 1\n",
    "        \n",
    "        # MAP\n",
    "        precisions11 = []\n",
    "        for idx2 in range(11):\n",
    "            if idx in results[0:idx2+1].index.tolist():\n",
    "                precisions11.append(1)\n",
    "            else:\n",
    "                precisions11.append(0)\n",
    "        average_precisions.append(np.mean(precisions11))\n",
    "\n",
    "\n",
    "        # print the progress made in precisions\n",
    "        if idx % 10 == 0:\n",
    "            print(f'Precision@5: {num_precision_5 / (idx + 1)}')\n",
    "            print(f'Precision@10: {num_precision_10 / (idx + 1)}')\n",
    "            print(f'Precision@20: {num_precision_20 / (idx + 1)}')\n",
    "            prec_5.append(num_precision_5 / (idx + 1))\n",
    "            prec_10.append(num_precision_10 / (idx + 1))\n",
    "            prec_20.append(num_precision_20 / (idx + 1))\n",
    "\n",
    "\n",
    "# compute precision@5, precision@10 and precision@20\n",
    "precision_5 = num_precision_5 / len(test['header'])\n",
    "precision_10 = num_precision_10 / len(test['header'])\n",
    "precision_20 = num_precision_20 / len(test['header'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Precision@5: {num_precision_5 / (idx + 1)}')\n",
    "print(f'Precision@10: {num_precision_10 / (idx + 1)}')\n",
    "print(f'Precision@20: {num_precision_20 / (idx + 1)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1942628137523729"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# MAP score\n",
    "np.mean(average_precisions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RUN ALL THIS BLOCK IF YOU WANT TO CHECK FOR MODEL PERFORMANCE\n",
    "* Note: before running this block, you need to insert the right paths for:\n",
    "    1. the test dataset\n",
    "    2. w2v model (all 3 files must be in the folder)\n",
    "    3. bert model (both model and classification head)\n",
    "    4. kdtree. \n",
    "* Note2: in order to shorter the time, this code only consider 100 questions from the test dataset. If you want to check the performance on the whole test dataset, please take into consideration that the code will take a lot of time to run (2 days, using a GPU accelerated environment)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import spacy\n",
    "spacy.prefer_gpu()\n",
    "import re\n",
    "import joblib\n",
    "import numpy as np\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import torch\n",
    "import nltk\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.neighbors import KDTree\n",
    "from transformers import BertModel, BertTokenizer\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "import torch\n",
    "import warnings\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "# SOSTITUIRE CON I PATH GIUSTI\n",
    "w2v_model = Word2Vec.load(r\"word2vec4.model\")\n",
    "test = pd.read_csv(r'test.csv')\n",
    "tree = joblib.load(\"kdtree_word2vec_from_scratch2.joblib\")\n",
    "# Load the model\n",
    "model = BertModel.from_pretrained('bert-base-uncased').to(device)\n",
    "model.load_state_dict(torch.load(r'model4.pt'))\n",
    "# Load the classification head\n",
    "classification_head = torch.nn.Sequential(torch.nn.Dropout(0.4), torch.nn.Linear(768, 1),torch.nn.Sigmoid()).to(device)\n",
    "classification_head.load_state_dict(torch.load(r'classification_head4.pt'))\n",
    "\n",
    "test_cases = 1000   # set to len(test) for complete evaluation\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Expand contractions\n",
    "\n",
    "contractions_dict = { \"ain't\": \"are not\",\"'s\":\" is\",\"aren't\": \"are not\",\"can't\": \"can not\",\"can't've\": \"cannot have\",\n",
    "\"'cause\": \"because\",\"could've\": \"could have\",\"couldn't\": \"could not\",\"couldn't've\": \"could not have\",\n",
    "\"didn't\": \"did not\",\"doesn't\": \"does not\",\"don't\": \"do not\",\"hadn't\": \"had not\",\"hadn't've\": \"had not have\",\n",
    "\"hasn't\": \"has not\",\"haven't\": \"have not\",\"he'd\": \"he would\",\"he'd've\": \"he would have\",\"he'll\": \"he will\",\n",
    "\"he'll've\": \"he will have\",\"how'd\": \"how did\",\"how'd'y\": \"how do you\",\"how'll\": \"how will\",\"i'd\": \"i would\",\n",
    "\"i'd've\": \"i would have\",\"i'll\": \"i will\",\"i'll've\": \"i will have\",\"i'm\": \"i am\",\"i've\": \"i have\",\n",
    "\"isn't\": \"is not\",\"it'd\": \"it would\",\"it'd've\": \"it would have\",\"it'll\": \"it will\",\"it'll've\": \"it will have\",\n",
    "\"let's\": \"let us\",\"ma'am\": \"madam\",\"mayn't\": \"may not\",\"might've\": \"might have\",\"mightn't\": \"might not\",\n",
    "\"mightn't've\": \"might not have\",\"must've\": \"must have\",\"mustn't\": \"must not\",\"mustn't've\": \"must not have\",\n",
    "\"needn't\": \"need not\",\"needn't've\": \"need not have\",\"o'clock\": \"of the clock\",\"oughtn't\": \"ought not\",\n",
    "\"oughtn't've\": \"ought not have\",\"shan't\": \"shall not\",\"sha'n't\": \"shall not\",\n",
    "\"shan't've\": \"shall not have\",\"she'd\": \"she would\",\"she'd've\": \"she would have\",\"she'll\": \"she will\",\n",
    "\"she'll've\": \"she will have\",\"should've\": \"should have\",\"shouldn't\": \"should not\",\n",
    "\"shouldn't've\": \"should not have\",\"so've\": \"so have\",\"that'd\": \"that would\",\"that'd've\": \"that would have\",\n",
    "\"there'd\": \"there would\",\"there'd've\": \"there would have\",\n",
    "\"they'd\": \"they would\",\"they'd've\": \"they would have\",\"they'll\": \"they will\",\"they'll've\": \"they will have\",\n",
    "\"they're\": \"they are\",\"they've\": \"they have\",\"to've\": \"to have\",\"wasn't\": \"was not\",\"we'd\": \"we would\",\n",
    "\"we'd've\": \"we would have\",\"we'll\": \"we will\",\"we'll've\": \"we will have\",\"we're\": \"we are\",\"we've\": \"we have\",\n",
    "\"weren't\": \"were not\",\"what'll\": \"what will\",\"what'll've\": \"what will have\",\"what're\": \"what are\",\n",
    "\"what've\": \"what have\",\"when've\": \"when have\",\"where'd\": \"where did\",\n",
    "\"where've\": \"where have\",\"who'll\": \"who will\",\"who'll've\": \"who will have\",\"who've\": \"who have\",\n",
    "\"why've\": \"why have\",\"will've\": \"will have\",\"won't\": \"will not\",\"won't've\": \"will not have\",\n",
    "\"would've\": \"would have\",\"wouldn't\": \"would not\",\"wouldn't've\": \"would not have\",\"y'all\": \"you all\",\n",
    "\"y'all'd\": \"you all would\",\"y'all'd've\": \"you all would have\",\"y'all're\": \"you all are\",\"y'all've\": \"you all have\",\n",
    "\"you'd\": \"you would\",\"you'd've\": \"you would have\",\"you'll\": \"you will\",\"you'll've\": \"you will have\",\n",
    "\"you're\": \"you are\",\"you've\": \"you have\"}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Regular expression for finding contractions\n",
    "contractions_re=re.compile('(%s)' % '|'.join(contractions_dict.keys()))\n",
    "\n",
    "# Function for expanding contractions\n",
    "def expand_contractions(text,contractions_dict=contractions_dict):\n",
    "    def replace(match):\n",
    "        return contractions_dict[match.group(0)]\n",
    "    return contractions_re.sub(replace, text)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def clean_text(text):\n",
    "    text = re.sub('\\w*\\d\\w*','', text)\n",
    "    #text=re.sub('\\n',' ',text)\n",
    "    text = re.sub(r\"http\\S+\", \"\", text)\n",
    "    text = re.sub('[^a-z]',' ',text)\n",
    "    return text\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm',disable=['ner','parser'])\n",
    "nlp.max_length = 5000000\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def get_embedding_w2v(doc_tokens, w2v_model=w2v_model):\n",
    "    embeddings = []\n",
    "    if len(doc_tokens)<1:\n",
    "        print(\"collection is empty\")\n",
    "        return np.zeros(500)\n",
    "    else:\n",
    "        for tok in doc_tokens:\n",
    "            try:\n",
    "                embeddings.append(w2v_model.wv[tok])\n",
    "                #print(\"embedding riuscito per: \", tok)\n",
    "            except:\n",
    "                embeddings.append(np.random.rand(500))\n",
    "                #print(\"embedding fallito per: \", tok)\n",
    "                \n",
    "        # mean the vectors of individual words to get the vector of the document\n",
    "        return np.mean(embeddings, axis=0) \n",
    "\n",
    "# Getting Word2Vec Vectors for Testing Corpus and Queries\n",
    "test['answer_vector'] = test['lemmatized_answer'].apply(lambda x : get_embedding_w2v(str(x).split(), w2v_model))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def ranking_ir(query, tree, data, k):\n",
    "  \n",
    "  # pre-process Query\n",
    "  query=query.lower().replace('\\n', '')\n",
    "  query=expand_contractions(query)\n",
    "  query=clean_text(query)\n",
    "  query=re.sub(' +',' ', query)\n",
    "  query = ' '.join([token.lemma_ for token in list(nlp(query)) if (token.is_stop==False)])\n",
    "\n",
    "  # generating vector\n",
    "  vector = get_embedding_w2v(query.split())\n",
    "\n",
    "  # ranking documents\n",
    "  dist, ind = tree.query(np.expand_dims(vector, axis = 0), k = k)   \n",
    "  \n",
    "  return data.iloc[ind.tolist()[0]][['cleaned_answer', 'tags']]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "\n",
    "\n",
    "# tokenizer \n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Put the model in evaluation mode--the dropout layers behave differently\n",
    "# during evaluation.\n",
    "model.eval()\n",
    "\n",
    "num_precision_5 = 0\n",
    "num_precision_10 = 0\n",
    "num_precision_20 = 0\n",
    "\n",
    "# precision lists\n",
    "prec_5 = []\n",
    "prec_10 = []\n",
    "prec_20 = []\n",
    "\n",
    "# import the test set\n",
    "#test = pd.read_csv('test.csv')\n",
    "average_precisions = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "\n",
    "    for idx, question in enumerate(test['header'][0:test_cases]):\n",
    "        # get the top 1000 results\n",
    "        results = ranking_ir(question, tree, test, 1000)\n",
    "\n",
    "        # take the questions and put them in a list\n",
    "        answers = results['cleaned_answer'].to_list()\n",
    "        \n",
    "        # create a list with 1000 times the same question\n",
    "        questions = [question] * 125\n",
    "\n",
    "        scores = []\n",
    "        for i in range(8):\n",
    "            encoding = tokenizer(\n",
    "                text = questions,\n",
    "                text_pair = answers[i*125:(i+1)*125],\n",
    "                add_special_tokens = True,\n",
    "                max_length = 512,\n",
    "                padding  = True,\n",
    "                return_attention_mask = True,\n",
    "                return_tensors = 'pt',\n",
    "                truncation = True\n",
    "            ).to(device)\n",
    "\n",
    "            ids = encoding['input_ids']\n",
    "            mask = encoding['attention_mask']\n",
    "            token_type_ids = encoding['token_type_ids']\n",
    "\n",
    "            output = model(ids, mask, token_type_ids)[0]\n",
    "            output = output[:, 0, :] + output[:, 1, :]\n",
    "            output = classification_head(output)\n",
    "            scores.append(output.squeeze().cpu().numpy())\n",
    "\n",
    "        scores = np.array(scores).flatten()\n",
    "        results['score'] = scores\n",
    "        results = results.sort_values(by=['score'], ascending=False)\n",
    "\n",
    "        # check if test['cleaned_answer'][idx] is in the top 5,10 or 20 results\n",
    "        if idx in results[0:5].index.tolist():\n",
    "            num_precision_5 += 1\n",
    "        if idx in results[0:10].index.tolist():\n",
    "            num_precision_10 += 1\n",
    "        if idx in results[0:20].index.tolist():\n",
    "            num_precision_20 += 1\n",
    "        \n",
    "        # MAP\n",
    "        precisions11 = []\n",
    "        for idx2 in range(11):\n",
    "            if idx in results[0:idx2+1].index.tolist():\n",
    "                precisions11.append(1)\n",
    "            else:\n",
    "                precisions11.append(0)\n",
    "        average_precisions.append(np.mean(precisions11))\n",
    "\n",
    "\n",
    "        # print the progress made in precisions\n",
    "        if idx % 10 == 0:\n",
    "            print(f'Precision@5: {num_precision_5 / (idx + 1)}')\n",
    "            print(f'Precision@10: {num_precision_10 / (idx + 1)}')\n",
    "            print(f'Precision@20: {num_precision_20 / (idx + 1)}')\n",
    "            prec_5.append(num_precision_5 / (idx + 1))\n",
    "            prec_10.append(num_precision_10 / (idx + 1))\n",
    "            prec_20.append(num_precision_20 / (idx + 1))\n",
    "\n",
    "\n",
    "# compute precision@5, precision@10 and precision@20\n",
    "precision_5 = num_precision_5 / len(test['header'])\n",
    "precision_10 = num_precision_10 / len(test['header'])\n",
    "precision_20 = num_precision_20 / len(test['header'])\n",
    "\n",
    "\n",
    "# print results\n",
    "print(f'Precision@5: {precision_5}')\n",
    "print(f'Precision@10: {precision_10}')\n",
    "print(f'Precision@20: {precision_20}')\n",
    "print(f'MAP: {np.mean(average_precisions)}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
