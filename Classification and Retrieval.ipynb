{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Mining Project\n",
    "### Retrieval\n",
    "Creo degli embedding per il testo delle risposte e delle domande, poi addestro una rete che vada ad allineare lo spazio degli embedding delle domande con quello delle risposte. In questo modo, dato un testo di input, la rete mi restituisce l'embedding più simile tra quelli delle risposte (più similmente, le k risposte più simili). Questo embedding è poi utilizzato per trovare la risposta più simile tra quelle disponibili.\n",
    "\"Pre-computed answer embeddings served as ground true labels. In this way, quesion encoder was forced to learn projecting a question into an embedding close to answer embedding\".\n",
    "\n",
    "### Classification\n",
    "\n",
    "#### Sviluppi futuri\n",
    "Inserire la data di pubblicazione della domanda e della risposta, per poter compredere a valutare l'attualità della soluzione proposta."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>codice sorgente</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>189602</td>\n",
       "      <td>[&lt;!DOCTYPE html&gt;\\n\\n&lt;html class=\"html__respons...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>189603</td>\n",
       "      <td>[&lt;!DOCTYPE html&gt;\\n\\n&lt;html class=\"html__respons...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>189604</td>\n",
       "      <td>[&lt;!DOCTYPE html&gt;\\n\\n&lt;html class=\"html__respons...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>189605</td>\n",
       "      <td>[&lt;!DOCTYPE html&gt;\\n\\n&lt;html class=\"html__respons...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>189606</td>\n",
       "      <td>[&lt;!DOCTYPE html&gt;\\n\\n&lt;html class=\"html__respons...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       id                                    codice sorgente\n",
       "0  189602  [<!DOCTYPE html>\\n\\n<html class=\"html__respons...\n",
       "1  189603  [<!DOCTYPE html>\\n\\n<html class=\"html__respons...\n",
       "2  189604  [<!DOCTYPE html>\\n\\n<html class=\"html__respons...\n",
       "3  189605  [<!DOCTYPE html>\\n\\n<html class=\"html__respons...\n",
       "4  189606  [<!DOCTYPE html>\\n\\n<html class=\"html__respons..."
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stack_data_raw = r\"E:\\J.A.R.V.I.S\\stack_data\\Codici Sorgente\\codici_8(189602-200000).csv\"\n",
    "stack_data_df = pd.read_csv(stack_data_raw)\n",
    "stack_data_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>header</th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "      <th>tags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>189602</td>\n",
       "      <td>[&lt;!DOCTYPE html&gt;\\n\\n&lt;html class=\"html__respons...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>189603</td>\n",
       "      <td>[&lt;!DOCTYPE html&gt;\\n\\n&lt;html class=\"html__respons...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>189604</td>\n",
       "      <td>[&lt;!DOCTYPE html&gt;\\n\\n&lt;html class=\"html__respons...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>189605</td>\n",
       "      <td>[&lt;!DOCTYPE html&gt;\\n\\n&lt;html class=\"html__respons...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>189606</td>\n",
       "      <td>[&lt;!DOCTYPE html&gt;\\n\\n&lt;html class=\"html__respons...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   header                                           question answer  tags\n",
       "0  189602  [<!DOCTYPE html>\\n\\n<html class=\"html__respons...   None  None\n",
       "1  189603  [<!DOCTYPE html>\\n\\n<html class=\"html__respons...   None  None\n",
       "2  189604  [<!DOCTYPE html>\\n\\n<html class=\"html__respons...   None  None\n",
       "3  189605  [<!DOCTYPE html>\\n\\n<html class=\"html__respons...   None  None\n",
       "4  189606  [<!DOCTYPE html>\\n\\n<html class=\"html__respons...   None  None"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stack_data_df['answer'] = None\n",
    "stack_data_df['tags'] = None\n",
    "\n",
    "stack_data_df = stack_data_df.rename(columns={'id': 'header', 'codice sorgente': 'question'})\n",
    "stack_data_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8935, 4)\n"
     ]
    }
   ],
   "source": [
    "print(stack_data_df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting relevant data from scraped web pages: 20k web pages --> 20m 47 sec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kevin\\AppData\\Local\\Temp\\ipykernel_50400\\3869076605.py:11: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  stack_data_df['answer'][idx] = answer.text\n",
      "C:\\Users\\kevin\\AppData\\Local\\Temp\\ipykernel_50400\\3869076605.py:19: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  stack_data_df['header'][idx] = header.text\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In this scraping process we have 19 of 10397 question/answer that were dropped because of missing information\n"
     ]
    }
   ],
   "source": [
    "missing_info = 0\n",
    "\n",
    "for idx,soup in enumerate(stack_data_df['question']):\n",
    "\n",
    "    soup = BeautifulSoup(soup,\"html.parser\")\n",
    "\n",
    "    try:\n",
    "      # Getting the answer\n",
    "      answer = soup.find('div', class_ = ['answer', 'js-answer'], itemprop='acceptedAnswer') \n",
    "      answer = answer.find('div', class_ = ['s-prose js-post-body'], itemprop='text')   \n",
    "      stack_data_df['answer'][idx] = answer.text\n",
    "\n",
    "    except:\n",
    "      missing_info+=1\n",
    "      continue\n",
    "    \n",
    "    # Getting the header\n",
    "    header = soup.find('h1', class_ = ['fs-headline1 ow-break-word mb8 flex--item fl1'], itemprop ='name')\n",
    "    stack_data_df['header'][idx] = header.text\n",
    "\n",
    "    # Getting the question\n",
    "    question = soup.find('div', class_ = ['question js-question'], id ='question') \n",
    "    question = question.find('div', class_ = ['s-prose js-post-body'], itemprop='text') \n",
    "    stack_data_df['question'][idx] = question.text\n",
    "\n",
    "    # Getting the tags\n",
    "    tags = soup.find('div', class_ = ['d-flex ps-relative fw-wrap'])\n",
    "    tags = tags.text.split()[1:]\n",
    "    # se è missing riempio con lista vuota\n",
    "    if len(tags) == 0:\n",
    "      tags = []\n",
    "    stack_data_df['tags'][idx] =  tags\n",
    "\n",
    "stack_data_df.to_csv(fr'C:\\Users\\kevin\\Desktop\\Text Mining data\\df_21.csv', index = False)\n",
    "print(f\"In this scraping process we have {missing_info} of {idx} question/answer that were dropped because of missing information\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contextual word embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertModel\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "\n",
    "# Loading the pre-trained BERT model\n",
    "###################################\n",
    "# Embeddings will be derived from\n",
    "# the outputs of this model\n",
    "model = BertModel.from_pretrained('bert-base-uncased',\n",
    "                                  output_hidden_states = True,\n",
    "                                  )\n",
    "\n",
    "# Setting up the tokenizer\n",
    "###################################\n",
    "# This is the same tokenizer that\n",
    "# was used in the model to generate \n",
    "# embeddings to ensure consistency\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = [\"bank\",\n",
    "         \"The river bank was flooded.\",\n",
    "         \"The bank vault was robust.\",\n",
    "         \"He had to bank on her for support.\",\n",
    "         \"The bank was out of money.\",\n",
    "         \"The bank teller was a man.\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bert_text_preparation(text, tokenizer):\n",
    "    \"\"\"Preparing the input for BERT\n",
    "    \n",
    "    Takes a string argument and performs\n",
    "    pre-processing like adding special tokens,\n",
    "    tokenization, tokens to ids, and tokens to\n",
    "    segment ids. All tokens are mapped to seg-\n",
    "    ment id = 1.\n",
    "    \n",
    "    Args:\n",
    "        text (str): Text to be converted\n",
    "        tokenizer (obj): Tokenizer object\n",
    "            to convert text into BERT-re-\n",
    "            adable tokens and ids\n",
    "        \n",
    "    Returns:\n",
    "        list: List of BERT-readable tokens\n",
    "        obj: Torch tensor with token ids\n",
    "        obj: Torch tensor segment ids\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    marked_text = \"[CLS] \" + str(text) + \" [SEP]\"\n",
    "    tokenized_text = tokenizer.tokenize(marked_text)\n",
    "    indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
    "    segments_ids = [1]*len(indexed_tokens)\n",
    "\n",
    "    # Convert inputs to PyTorch tensors\n",
    "    tokens_tensor = torch.tensor([indexed_tokens])\n",
    "    segments_tensors = torch.tensor([segments_ids])\n",
    "\n",
    "    return tokenized_text, tokens_tensor, segments_tensors\n",
    "    \n",
    "def get_bert_embeddings(tokens_tensor, segments_tensors, model):\n",
    "    \"\"\"Get embeddings from an embedding model\n",
    "    \n",
    "    Args:\n",
    "        tokens_tensor (obj): Torch tensor size [n_tokens]\n",
    "            with token ids for each token in text\n",
    "        segments_tensors (obj): Torch tensor size [n_tokens]\n",
    "            with segment ids for each token in text\n",
    "        model (obj): Embedding model to generate embeddings\n",
    "            from token and segment ids\n",
    "    \n",
    "    Returns:\n",
    "        list: List of list of floats of size\n",
    "            [n_tokens, n_embedding_dimensions]\n",
    "            containing embeddings for each token\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    # Gradient calculation id disabled\n",
    "    # Model is in inference mode\n",
    "    with torch.no_grad():\n",
    "        outputs = model(tokens_tensor, segments_tensors)\n",
    "        # Removing the first hidden state\n",
    "        # The first state is the input state\n",
    "        hidden_states = outputs[2][1:]\n",
    "\n",
    "    # Getting embeddings from the final BERT layer\n",
    "    token_embeddings = hidden_states[-1]\n",
    "    # Collapsing the tensor into 1-dimension\n",
    "    token_embeddings = torch.squeeze(token_embeddings, dim=0)\n",
    "    # Converting torchtensors to lists\n",
    "    list_token_embeddings = [token_embed.tolist() for token_embed in token_embeddings]\n",
    "\n",
    "    return list_token_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting embeddings for the target\n",
    "# word in all given contexts\n",
    "target_word_embeddings = []\n",
    "\n",
    "# texts è una lista di stringhe\n",
    "for text in texts:\n",
    "    tokenized_text, tokens_tensor, segments_tensors = bert_text_preparation(text, tokenizer)\n",
    "    \n",
    "    list_token_embeddings = get_bert_embeddings(tokens_tensor, segments_tensors, model)\n",
    "    \n",
    "    # Find the position 'bank' in list of tokens\n",
    "    word_index = tokenized_text.index('bank')\n",
    "    # Get the embedding for bank\n",
    "    word_embedding = list_token_embeddings[word_index]\n",
    "\n",
    "    target_word_embeddings.append(word_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "583 torch.Size([1, 583]) torch.Size([1, 583])\n"
     ]
    }
   ],
   "source": [
    "print(len(tokenized_text), tokens_tensor.shape, segments_tensors.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>header</th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "      <th>tags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>How to find output instead of dimensions in ke...</td>\n",
       "      <td>\\nI tried to get the output of following code,...</td>\n",
       "      <td>\\nFor that, use get_value:\\nimport numpy as np...</td>\n",
       "      <td>['numpy', 'keras', 'deep-learning']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>python, measure unique elements by row ignorin...</td>\n",
       "      <td>\\nI have a data frame of 6 columns where each ...</td>\n",
       "      <td>\\nYou can change myfunc to \\ndef myfunc(row):\\...</td>\n",
       "      <td>['unique', 'counting']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>python division with large numbers automatical...</td>\n",
       "      <td>\\nPython 3.7.4 (tags/v3.7.4:e09359112e, Jul  8...</td>\n",
       "      <td>\\nUse floor division:\\n&gt;&gt;&gt; 111111111111111111/...</td>\n",
       "      <td>['division']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>speech recognition in PyCharm on Mac missing P...</td>\n",
       "      <td>\\n#raise AttributeError(\"Could not find PyAudi...</td>\n",
       "      <td>\\nYou may need to get some additional packages...</td>\n",
       "      <td>['pyaudio']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>How to refer to a class list item?</td>\n",
       "      <td>\\nI'm a beginner at Python and I don't know wh...</td>\n",
       "      <td>\\ncommand should be set to a callback function...</td>\n",
       "      <td>['tkinter', 'tk']</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              header  \\\n",
       "0  How to find output instead of dimensions in ke...   \n",
       "1  python, measure unique elements by row ignorin...   \n",
       "2  python division with large numbers automatical...   \n",
       "3  speech recognition in PyCharm on Mac missing P...   \n",
       "4                 How to refer to a class list item?   \n",
       "\n",
       "                                            question  \\\n",
       "0  \\nI tried to get the output of following code,...   \n",
       "1  \\nI have a data frame of 6 columns where each ...   \n",
       "2  \\nPython 3.7.4 (tags/v3.7.4:e09359112e, Jul  8...   \n",
       "3  \\n#raise AttributeError(\"Could not find PyAudi...   \n",
       "4  \\nI'm a beginner at Python and I don't know wh...   \n",
       "\n",
       "                                              answer  \\\n",
       "0  \\nFor that, use get_value:\\nimport numpy as np...   \n",
       "1  \\nYou can change myfunc to \\ndef myfunc(row):\\...   \n",
       "2  \\nUse floor division:\\n>>> 111111111111111111/...   \n",
       "3  \\nYou may need to get some additional packages...   \n",
       "4  \\ncommand should be set to a callback function...   \n",
       "\n",
       "                                  tags  \n",
       "0  ['numpy', 'keras', 'deep-learning']  \n",
       "1               ['unique', 'counting']  \n",
       "2                         ['division']  \n",
       "3                          ['pyaudio']  \n",
       "4                    ['tkinter', 'tk']  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nFor that, use get_value:\\nimport numpy as np\\nimport keras\\nfrom keras.layers import concatenate\\nimport tensorflow as tf\\n\\na=np.array([1,2,3])\\nb=np.array([4,5,6])\\nc=np.array([7,8,9])\\na = keras.backend.variable(a)\\nb = keras.backend.variable(b)\\nc = keras.backend.variable(c)\\nmerged_vector = concatenate([a,b,c], axis=-1)\\nprint(keras.backend.get_value(merged_vector))\\n\\n'"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['answer'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "answer_embs = []\n",
    "for idx,answer in enumerate(df['answer'][0:5000]):\n",
    "    try:\n",
    "        tokenized_text, tokens_tensor, segments_tensors = bert_text_preparation(str(answer), tokenizer)\n",
    "        #print(len(tokenized_text), tokens_tensor.shape, segments_tensors.shape)\n",
    "        list_token_embeddings = get_bert_embeddings(tokens_tensor, segments_tensors, model)\n",
    "        centroid = np.mean(list_token_embeddings, axis=0)[None,]\n",
    "        answer_embs.append(centroid.flatten())\n",
    "    except Exception as e:\n",
    "        pass     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KDTree\n",
    "import joblib\n",
    "tree = KDTree(answer_embs) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['kdtree_0_5000.joblib']"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(tree, \"kdtree_0_5000.joblib\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24.369444444444447"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tree = joblib.load(\"cat_kdtree.joblib\")\n",
    "tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8480.565371024735"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dist, ind = tree.query(np.expand_dims(qfeatures, axis = 0), k= 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2400"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kimage.load_img(paths[ind[0][0]])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6defd577cd85e3649e86c46a537635b7104b081260a238509c81cac8b534171b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
